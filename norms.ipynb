{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning something new, especially something as deep as machine learning (no pun intended...), can be daunting. And there are many terms and concepts that are used that can be foreign at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of these notions was the use of different \"norms\" when training models. I was familiar with norms from my math studies, but why and how how they were being used in machine learning, I hadn't a clue. So I dedicated some time to try to gain some understanding. Below is a summary of what I've found; perhaps it can be useful to someone else also starting out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bear in mind that while I don't really want to make this too heavy on the math and start throwing around a bunch of symbols, we can't avoid it all together. We'll need to have a few definitions under our belt, but I'll try to make it as painless as possible... promise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok... So what's a norm?\n",
    "\n",
    "Basically, a norm is a way of measuring distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might seem like a strange thing to say; I mean, isn't there just one distance? Well, sure, there's the one that every one learns in school based on the good old Pythagorean theorem: $a^2 + b^2 = c^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But using other ways of measuring a distance isn't really that weird if you think about it. In fact, you've probably used different norms\n",
    "quite a number of times. Imagine you are downtown of some major city and you need to go three streets down and four streets over to grab some lunch. You won't use the Pythagorean theorem to measure the distance you need to walk, right? I mean you can't exactly walk through all the buildings in your way... (unless you're really lucky and there's a bunch of empty lots between you and where you want to go). Instead, you'd say you have about seven blocks to walk and off you'd go. Without realizing it, you have used a different way of measuring distance. One that makes more sense in your context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is exactly what mathematicians, data scientists, or really anyone is doing when they choose a norm; they are picking an appropriate method to measure distance for their purpose.\n",
    "\n",
    "Ok, great, but what *is* a norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at [wikipedia](https://en.wikipedia.org/wiki/Norm_(mathematics)) for a formal definition, some properties, and some examples. But there is a group of norms that I'd like focus on here: the $l_p$ norms. For this, we'll need a definition... Let's suppose that we have a group or collection of numbers: $x_1, x_2, \\ldots, x_n$, and let's view them as an $n$ dimensional vector $\\bar{x} = \\left( x_1, x_2, \\ldots, x_n \\right)$. Then the $l_p$ norm is calculated by using the equation\n",
    "\n",
    "$$ \\left\\lVert \\bar{x} \\right\\lVert_{p} = \\left( \\left| x_{1} \\right|^{p} + \\left| x_{2} \\right|^{p} + \\ldots + \\left| x_{n} \\right|^{p} \\right)^{\\dfrac{1}{p}} = \\left( \\sum_{i = 1}^{n} \\left| x_{i} \\right|^{p} \\right)^{\\dfrac{1}{p}}$$\n",
    "\n",
    "where $0 < p < \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because this isn't already enough to take in, we can also define our norm for $p = \\infty$.\n",
    "\n",
    "$$ \\left\\lVert x \\right\\lVert_{\\infty} = \\max_{1 \\leq i \\leq n} \\{ x_{1}, x_{2}, \\ldots, x_{n} \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably a lot to take in, so let's break it down a little. First, let's just put $n = 2$ and work in two dimensions $x = \\left( x_{1}, x_{2} \\right)$. If $p = 2$, then the $l_{2}$ distance is the familiar notion of distance we all think about from middle school, and the formula to calculate it is the Pythagorean theorem!\n",
    "\n",
    "$$ \\left| x \\right|_{2} = \\sqrt{ \\left|x_{1} \\right|^{2} + \\left| x_{2} \\right|^{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool... so this is just a way to generalize distance from something that we already know. But what role does $p$ play when it isn't 2? How does changing it affect how the distance is being measured? To try to visualize this, let's plot the different circles for different values of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from geometry that a circle is just the collection of all points that are the same distance from a fixed point. As our idea of distance changes, then we'd expect that how the circle looks will change as well. So if we set our fixed point to be the origin, and plot the circle with radius $1$ for different values of $p$. What do they look like? Let's take a look..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Circles](circles.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's digest this a little and make some observations:\n",
    "\n",
    "\n",
    "1. Notice that the familiar case of $p = 2$ is in red and kept on the image for reference.\n",
    "\n",
    "\n",
    "2. The values on the axis stay fixed as the circles change shape. This means they always have the same distance for any $p$!\n",
    "\n",
    "\n",
    "3. As $p$ reduces in value from $p = 2$ to $p = 1$, the traditional circle transforms into a diamond, and as $p$ continues to get smaller looking more like a start until finally, for very small values of $p$ it becomes an almost perfect $+$ symbol. So these are all circles? Sure! We've just changed how we measure distance... Furthermore, the shape that these $l_{p}$ circles take when $p \\leq 1$ is actually a significant observation that we'll come back to a little later.\n",
    "\n",
    "\n",
    "4. As $p$ increases from $p = 2$ towards $\\infty$, the circle starts to balloon outward and begins to look like a square with rounded corners, and when $p$ \"hits\" $\\infty$, it is a full square. So a square is also circle? I'll leave that thought with you...\n",
    "\n",
    "\n",
    "5. I know I promised not to but too much math... But hey, I'm a math guy at heart and I can't resist... This animation that we used to view the different $l_{p}$ circles is actually illustrating a homotopy among the different circles for different $p$. This is a topic from algebraic topology, and if you're interested, you can check out [wikipedia](https://en.wikipedia.org/wiki/Homotopy) for more info. They have a really nice animation that shows how you can view a donut being transformed into a coffee mug...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok... so how are they used in machine learning?\n",
    "\n",
    "Mostly all the different models used in machine learning share some similar DNA... they utilize [numerical optimization techniques](https://en.wikipedia.org/wiki/Mathematical_optimization) in order to ensure that they are doing what they were designed to do as best they can. And generally, this is done by setting up some kind of measure (called an *objective function*, *cost function*, etc) that quantifies the model's performance and also allows the computer running the model to modify it in such a way as to improve the model's performance. Many times (not always, mind you), the objective function utilizes one of our $l_{p}$ norms to measure the \"distance\" that the model is away from its desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all sounds a bit abstract, so let's try to get our hands dirty a little bit and pose a simple problem to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember those numbers we had, $\\left( x_1, x_2, \\ldots, x_n \\right)$? Let's see if we can find a single number that is closest to all of them. To do that, we use our $l_{p}$ distances and define an objective function,\n",
    "\n",
    "$$ f_{p}(x) = \\left( \\left| x_{1} - x \\right|^{p} + \\left| x_{2} - x\\right|^{p} + \\ldots + \\left| x_{n} - x \\right|^{p} \\right)^{\\dfrac{1}{p}} = \\left( \\sum_{i = 1}^{n} \\left| x_{i} - x \\right|^{p} \\right)^{\\dfrac{1}{p}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that $f$ just measures how far a number $x$ is from all of our numbers. To try to find the $x$ that is closest to all of the $x_{i}$, we just need to find the value of $x$ that makes $f$ as small as possible. If we do that, then we'll have the number that is as close as possible to each $x_{i}$ since $f$ is measuring a distance. But $f$ also needs a choice of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these objective functions to see what we expect the solutions to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lp_costs](lp_cost.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we record the minimum value for each of these graphs, and plot them, then we'd get something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![minimizers](lp_min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how unstable the graph of $f_p$ looks when $p$ is small (say $p < 1$); this is reflected on the graph of solutions as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing $f_p$ by using graphs like this is useful to build some intuition (or gain some understanding), and it works well in our case because\n",
    "\n",
    "* we've limited the dimension of the problem \n",
    "* the problem itself isn't too complicated\n",
    "* we aren't using too many points\n",
    "\n",
    "But for more complicated problems, using a graphical method isn't practical: we need a more robust methodology. This is where numerical optimization libraries (also called solvers) come in. They are purpose built software libraries that implement algorithms designed to help solve often very complex problems by minimizing (or maximizing) an objective (usually with additional constraints, but we'll skip that for now). \n",
    "\n",
    "Let's try to use a solver to minimize our objective functions and compare our results to what we got graphically. There are a wide range of solvers available, we with their own pros/cons and use cases. For this example, we'll use a decent (albiet slow) general all-purpose (non-linear) algorithm: the [Nelder–Mead method](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) (or the amoeba method). To do this, let's break out the old Python and write some code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to import a few modules to help us out: Numpy and SciPy's optimize sub-module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we setup a collection of numbers to use for our test. I'll also seed NumPy's random engine so we can get reproducible results. Notice that I sample an exponential distribution to get the numbers. This is because the exponential distribution spreads the numbers out enough to make the solution to our problem, for different $p$ values, different enough to be significant. But there are many, *many* different ways you could get your numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 101\n",
    "\n",
    "np.random.seed(111)\n",
    "b = np.random.exponential(10.5, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![numbers](numbers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some objective functions. To keep things simple, we leave out any error checking and the like and ignore $p = \\infty$. We also define some options to pass to our solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(numbers, p):\n",
    "    return lambda x : np.power(np.sum(np.power(np.abs(numbers - x), p)), 1 / p)\n",
    "\n",
    "options = {'maxiter': 200, 'disp': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to minimize our cost (or objective) function when $p = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min = 5.7435\n"
     ]
    }
   ],
   "source": [
    "lp_min_05 = 0.0 # We initialize our guess to 0.\n",
    "lp_min_05 = minimize(objective_function(b, 0.5), lp_min_05, \n",
    "                     method = 'Nelder-Mead', options = options).x[0] # Use 'Nelder-Mead'\n",
    "print('min = {:0.4f}'.format(lp_min_05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the solver returns a value of 5.7435; this is really close to the value we expected by reading the graph! Now let's try $p = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min = 6.5916\n"
     ]
    }
   ],
   "source": [
    "lp_min_1 = 0.0 # We initialize our guess to 0.\n",
    "lp_min_1 = minimize(objective_function(b, 1.0), lp_min_1, \n",
    "                     method = 'Nelder-Mead', options = options).x[0] # Use 'Nelder-Mead'\n",
    "print('min = {:0.4f}'.format(lp_min_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this matches what we expected from our graph. Lastly, let's try $p = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min = 10.6751\n"
     ]
    }
   ],
   "source": [
    "lp_min_2 = 0.0 # We initialize our guess to 0.\n",
    "lp_min_2 = minimize(objective_function(b, 2.0), lp_min_2, \n",
    "                     method = 'Nelder-Mead', options = options).x[0] # Use 'Nelder-Mead'\n",
    "print('min = {:0.4f}'.format(lp_min_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another match!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try to minimize $f_p$ by hand for some particular values of $p$: $p = 1,2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $p = 2$, we want to minimize \n",
    "$$ f_{2}(x) = \\left( \\sum_{i = 1}^{n} \\left| x_{i} - x \\right|^{2} \\right)^{\\dfrac{1}{2}} = \\left( \\sum_{i = 1}^{n} \\left( x_{i} - x \\right)^{2} \\right)^{\\dfrac{1}{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from calculus, that if $x$ minimizes $f_p$, then $x$ is a critical point, so we can take the derivative of $f_p$ and set it equal to 0 to find candidate values.\n",
    "\n",
    "__Note__: We could also minimize $f_{p}^{2}$ (which is more common and can simplify the calculations), but we will use $f_p$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now $$ \\begin{align} \\dfrac{d}{dx} f_{p}\\left(x\\right) &= \\dfrac{d}{dx} \\left( \\sum_{i = 1}^{n} \\left( x_{i} - x \\right)^{2} \\right)^{\\dfrac{1}{2}} \\\\ &= \\dfrac{1}{2} \\left( \\sum_{i = 1}^{n} \\left( x_{i} - x \\right)^{2} \\right)^{-\\dfrac{1}{2}} \\left( \\sum_{i = 1}^{n} \\left( x_{i} - x \\right) \\right) \\\\ \\\\ &= \\dfrac{\\sum_{i = 1}^{n} \\left( x_{i} - x \\right)}{2 \\sqrt{ \\sum_{i = 1}^{n} \\left( x_{i} - x \\right)^{2}}}  \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the critical points, we need to find when the derivative is either 0 or undefined, so we need to solve\n",
    "$$ \\sum_{i = 1}^{n} \\left( x_{i} - x \\right) = 0 $$\n",
    "and \n",
    "$$ \\sqrt{ \\sum_{i = 1}^{n} \\left( x_{i} - x \\right)^{2} } = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at second equation,\n",
    "$$ \\begin{align} \\sqrt{ \\sum_{i = 1}^{n} \\left( x_{i} - x \\right)^{2} } &= 0 \\\\ \\sum_{i = 1}^{n} \\left( x_{i} - x \\right)^{2} &= 0 \\end{align}$$\n",
    "Notice that the left-hand side of the equation is the sum of terms which are each non-negative. Thus, the sum can only be 0 if each term is 0. In other words, we would need to have $$\\left( x_{i} - x \\right)^{2} = 0, \\space 1 \\leq i \\leq n$$ or equivalently, $$ x = x_{i}, \\space 1 \\leq i \\leq n$$ which implies that all of the $x_{i}$ are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's solve the first equation,\n",
    "$$ \\begin{align} 0 &= \\sum_{i = 1}^{n} \\left( x_{i} - x \\right) \\\\ 0 &= \\sum_{i = 1}^{n} x_{i} - \\sum_{i = 1}^{n} x  \\\\ \\sum_{i = 1}^{n} x &= \\sum_{i = 1}^{n} x_{i} \\\\ n \\cdot x &= \\sum_{i = 1}^{n} x_{i} \\\\ x &= \\dfrac{1}{n} \\sum_{i = 1}^{n} x_{i} \\end{align} $$\n",
    "Which is the average (mean) of $x_1,\\ldots,x_n$. So minimizing the $l_2$ distance of our numbers $x_1,\\ldots,x_n$ is one way to realize (or motivate) the familiar average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to minimize $f_1$ by hand\n",
    "$$ f_{1}(x) = \\sum_{i = 1}^{n} \\left| x_{i} - x \\right| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to take the derivative of our objective function. But before we do that, since our objective function involves absolut values, let's reacall that\n",
    "\n",
    "$$ \\dfrac{d}{dx} \\left| x \\right| = \\left \\{ \\begin{matrix} -1, & x < 0 \\\\ 1, & x > 0 \\end{matrix} \\right. $$\n",
    "\n",
    "![abs](abs_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \n",
    "$$ \\dfrac{d}{dx} \\sum_{i = 1}^{n} \\left| x_{i} - x \\right| = \\sum_{\\{i : x_i \\leq x \\}} -1 + \\sum_{\\{i : x_i > x\\} } 1$$\n",
    "\n",
    "We may notice that a detail is off, namely, I'm ignoring the fact that the derivative doesn't exist for the $i$-th term if $x_i = x$; we'll just side step this for now and continue ahead. \n",
    "\n",
    "Now let's order and relabel the numbers $x_1,x_2,\\ldots,x_n$ and let $j$ be such that $x_i \\leq x$ for all $i \\leq j$ and $x_i > x$ for all $i > j$. Then we can write\n",
    "\n",
    "$$ \\dfrac{d}{dx} \\sum_{i = 1}^{n} \\left| x_{i} - x \\right| = \\sum_{i=1}^{j} -1 + \\sum_{i = j+1}^{n} 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative equal to zero, \n",
    "$$\\begin{align} 0 &= \\sum_{i=1}^{j} -1 + \\sum_{i = j+1}^{n} 1  \\\\ \\sum_{i=1}^{j} 1 &= \\sum_{i = j+1}^{n} 1 \\end{align}$$\n",
    "we can see that we need the total number of 1's on the left hand side to equal the total number of 1's on the right; this can only happen if $x$ is exactly in the middle of the $x_1,x_2,\\ldots,x_n$.\n",
    "\n",
    "This is a little less straight-forward than the case $p=2$ that we saw before, so let's recap. If we want to minimize $f_1$ (that is, find the point which makes the derivative 0), we need to \n",
    "\n",
    "1. Order the numbers $x_1,x_2,\\ldots,x_n$\n",
    "\n",
    "\n",
    "2. Find the value in exactly in the middle\n",
    "\n",
    "But this is exactly the process for finding the median of $x_1,x_2,\\ldots,x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's review:\n",
    "\n",
    "* If we want to minimize the $l_2$ distance from the numbers $x_1,x_2,\\ldots,x_n$, we get the mean.\n",
    "\n",
    "\n",
    "* If we want to minimize the $l_1$ distance from the numbers $x_1,x_2,\\ldots,x_n$, we get the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use all of the info above to make a few observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 1 - Sensitivity to Outliers\n",
    "\n",
    "Let remember our objective function for the $f_p$ norm of $\\bar{x} = \\left( x_1,x_2,\\ldots,x_n \\right)$\n",
    "\n",
    "$$ f_{p}(x) = \\left( \\sum_{i = 1}^{n} \\left| x_{i} - x \\right|^{p} \\right)^{\\dfrac{1}{p}} $$\n",
    "\n",
    "If we have outliers among the values $x_i$ (values very far from the rest of the points), then their individual distance from the minimum, $\\left| x - x_i \\right|$ is larger than the others. So\n",
    "\n",
    "* If $p > 1$, then raising the distance to the $p$-th power would eggagerate this large value and make the objective function larger. In order to compensate for this, the value that minimizes $f_p$ needs to be brought closer to this outlier in order to keep $f_p$ as small as possible.\n",
    "\n",
    "\n",
    "* If $p <= 1$, then raising the distance to the $p$-th power would reduces this large value and doesn't make a big contribution to the objective function. Thus, the value that minimizes $f_p$ will more closely represent the majority of the $x_{i}$'s.\n",
    "\n",
    "We can make this more concrete with an example; remember that the minimum of $f_2$ is the mean and the minimum of $f_1$ is the median. Let's say that you go out to eat with two friends, and you order a $\\$12$ plate, one friend orders a $\\$15$ plate, and the other friends goes crazy and orders a $\\$90$ plate. When you split the bill at the end, each of you would pay the mean price per plate, $\\$39$; while the median price, $\\$15$, isn't as affected by the crazy friend. If instead, your crazy friend had ordered something a cheaper, maybe a $\\$18$ plate, then the mean price would drop to $\\$15$ while the median price doesn't change.\n",
    "\n",
    "This sensitivity to outliers (especially for $p = 2$) is one reason why regularization is so important in numerical optimization; the regularization can help to reduce the impact of outliers or other noise on the solution.\n",
    "\n",
    "This illustrates how the different norms are affected by outliers in a collection of values. It might be tempting to then say, why not just always use $p=1$ if it is less sensitive to outliers? But it is not as stright-forward as that. Firstly, there might be cases where you may want to use $p=2$l for example, when using the average to split a bill. But there is another aspect to consider, some values of $p$ are easier to work with and require less computer time and processor power to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 2 - Computational Expense\n",
    "\n",
    "Instead of trying to explain different optimization algorithms in detail or give a theoretical description, let me try to give some motivation to this idea. If we think back to our examples for $p=2$ and $p=1$, The value that minimizes the $l_2$ norm is the mean. To calculate the mean, you need to add up all of the numbers and divide by the total number; this is pretty straight-forward and fast to do. If you organize your numbers contiguously in the computer's memory, then different processor optimizations can make this extremelt fast. In contrast, for a computer to compute the median (naively), you first have to sort all the values which require more computations to accomplish. So the time and effort needed to compute the median is greater, so the potential benefits that using an $l_1$ norm may bring to your problem are balanced by the additional time or resources that are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 3 - Sparseness Promoting (bonus)\n",
    "\n",
    "Again, rather than trying to give a rigorous justification of this; we'll try to gain some intuition. When we think about what a minimization problem involving $x_1,x_2,\\ldots,x_n$, one way to think about it is that we are trying to find a value on an $l_p$ circle (in $n$ dimensional space) constrined by some condtitions (our objective function) whose radius is as small as possible. And typically, a solver will start with an initial guess at the solution (whose cost is large) and iteratively update this value in such a way to reduce the objective function.\n",
    "\n",
    "If we look back at our circles plot and freeze it for $p=2$, then the circle is spread out evenly in all directions and any point on the circle seems just as good as any other.\n",
    "\n",
    "But if we instead use $p < 1$ (say $p = 0.75$), then the circle becomes more star like with the points aligned with the coordinate axes, and it seems reasonable that a solution is moer likely to settle on one of those point.  But at these point, we have the most 0 entries that we can have for a non-zero vector; in other words, those points are sparse.\n",
    "\n",
    "It has become a common practice in some disciplines to advertise their solvers/inversion schemes as sparse, but it may not always be apparent why thats a good thing. One place where a sparsity promoting method might be desierable is when a [pre-conditioner](https://en.wikipedia.org/wiki/Preconditioner) is used in order to improve the performance of the solver. If a transformation is used as part of the pre-conditioning in which the data is naturally sparse (e.g. a wavelet, curvlet, $\\tau$-p transform, etc), then having a solver which preseves this sparseness would be ideal as it may minimize your objective function in a way that reduces leakage or more closely resembles the desired data.\n",
    "\n",
    "Also, in the context of machine learning, this property can be used in cost functions or in regularization terms (as in Lasso regression) to help reduce weak features and play the role of a kind of feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of sums up the results of my effort to try to explain the idea behind the commonly used $l_p$ norms and they role they can play when in numerical optimization. I thought I'd share these thoughts; hopefully, they are beneficial and haven't left you more confused than when you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A blank screen...](https://media.giphy.com/media/KeKOZPh6kIHkUHSkOX/giphy.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
